{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-BJRVRTC3.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyAppName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=MyAppName>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc=pyspark.SparkContext(appName=\"MyAppName\")\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+------------------+\n",
      "|Pid|P_Name|Price|  Price_Normalized|\n",
      "+---+------+-----+------------------+\n",
      "|101|    P1|  450|               0.0|\n",
      "|102|    P2| 4034|0.8258064516129032|\n",
      "|103|    P3| 4790|               1.0|\n",
      "+---+------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max\n",
    "\n",
    "# Step 1: Start Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Price Normalization\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Sample data\n",
    "Product_Data = [(101, \"P1\", 450), (102, \"P2\", 4034), (103, \"P3\", 4790)]\n",
    "columns = [\"Pid\", \"P_Name\", \"Price\"]\n",
    "Product_DF = spark.createDataFrame(Product_Data, columns)\n",
    "\n",
    "# Step 3: Compute min and max\n",
    "Price_min = Product_DF.select(min(col(\"Price\"))).collect()[0][0]\n",
    "Price_max = Product_DF.select(max(col(\"Price\"))).collect()[0][0]\n",
    "\n",
    "# Step 4: Apply Min-Max Normalization\n",
    "Product_DF_normalized = Product_DF.withColumn(\n",
    "    \"Price_Normalized\", \n",
    "    (col(\"Price\") - Price_min) / (Price_max - Price_min)\n",
    ")\n",
    "\n",
    "# Step 5: Show result\n",
    "Product_DF_normalized.show()\n",
    "\n",
    "# Step 6: Stop Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------------------+\n",
      "|Pid|P_Name|Price| Price_Standardized|\n",
      "+---+------+-----+-------------------+\n",
      "|101|    P1|  450|-1.1392504769565068|\n",
      "|102|    P2| 4034|0.40658762605161547|\n",
      "|103|    P3| 4790| 0.7326628509048912|\n",
      "+---+------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Standardize Price\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sample data\n",
    "Product_Data = [(101, \"P1\", 450), (102, \"P2\", 4034), (103, \"P3\", 4790)]\n",
    "columns = [\"Pid\", \"P_Name\", \"Price\"]\n",
    "\n",
    "Product_DF = spark.createDataFrame(Product_Data, columns)\n",
    "\n",
    "# Calculate Mean and Standard Deviation\n",
    "Price_mean = Product_DF.select(mean(col(\"Price\"))).collect()[0][0]\n",
    "Price_stddev = Product_DF.select(stddev(col(\"Price\"))).collect()[0][0]\n",
    "\n",
    "# Standardize\n",
    "Product_DF_standardized = Product_DF.withColumn(\n",
    "    \"Price_Standardized\",\n",
    "    (col(\"Price\") - Price_mean) / Price_stddev\n",
    ")\n",
    "\n",
    "# Show result\n",
    "Product_DF_standardized.show()\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Salary_Grade|\n",
      "+------------+\n",
      "|     Class A|\n",
      "|     Class B|\n",
      "|     Class C|\n",
      "|     Class C|\n",
      "|     Class A|\n",
      "|     Class B|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyAppName\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "salarydata = [(\"Class A\",), (\"Class B\",), (\"Class C\",), (\"Class C\",), (\"Class A\",), (\"Class B\",)]\n",
    "columns = [\"Salary_Grade\"]\n",
    "\n",
    "salary_df = spark.createDataFrame(salarydata, columns)\n",
    "salary_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|Salary_Grade|Salary_Grade_Index|\n",
      "+------------+------------------+\n",
      "|     Class A|               0.0|\n",
      "|     Class B|               1.0|\n",
      "|     Class C|               2.0|\n",
      "|     Class C|               2.0|\n",
      "|     Class A|               0.0|\n",
      "|     Class B|               1.0|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#label encoding\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Initialize StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Salary_Grade\", outputCol=\"Salary_Grade_Index\")\n",
    "\n",
    "# Fit and Transform the DataFrame\n",
    "salary_df_indexed = indexer.fit(salary_df).transform(salary_df)\n",
    "salary_df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+---------------------------+\n",
      "|Salary_Grade|Salary_Grade_Index|Salary_Grade_OneHotEncoding|\n",
      "+------------+------------------+---------------------------+\n",
      "|     Class A|               0.0|              (2,[0],[1.0])|\n",
      "|     Class B|               1.0|              (2,[1],[1.0])|\n",
      "|     Class C|               2.0|                  (2,[],[])|\n",
      "|     Class C|               2.0|                  (2,[],[])|\n",
      "|     Class A|               0.0|              (2,[0],[1.0])|\n",
      "|     Class B|               1.0|              (2,[1],[1.0])|\n",
      "+------------+------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#one hot encoding\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCol=\"Salary_Grade_Index\", outputCol=\"Salary_Grade_OneHotEncoding\")\n",
    "\n",
    "# Transform the DataFrame\n",
    "salary_df_encoded = encoder.fit(salary_df_indexed).transform(salary_df_indexed)\n",
    "salary_df_encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------------+\n",
      "|Pid|P_Name|Price|Price_Squared|\n",
      "+---+------+-----+-------------+\n",
      "|101|    P1|  450|     202500.0|\n",
      "|102|    P2| 4034|  1.6273156E7|\n",
      "|103|    P3| 4790|    2.29441E7|\n",
      "+---+------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"PolynomialFeature\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "Product_Data = [(101, \"P1\", 450), (102, \"P2\", 4034), (103, \"P3\", 4790)]\n",
    "columns = [\"Pid\", \"P_Name\", \"Price\"]\n",
    "Product_DF = spark.createDataFrame(Product_Data, columns)\n",
    "\n",
    "# Feature Engineering: Create Price^2\n",
    "df_new = Product_DF.withColumn(\"Price_Squared\", col(\"Price\") ** 2)\n",
    "\n",
    "# Show results\n",
    "df_new.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------+-----------------+\n",
      "|Pname|Units_Sold|Price_Per_Unit|Total_Sales_Value|\n",
      "+-----+----------+--------------+-----------------+\n",
      "|   P1|         1|           500|              500|\n",
      "|   P2|         2|           200|              400|\n",
      "|   P3|         3|           300|              900|\n",
      "|   P4|         4|           400|             1600|\n",
      "+-----+----------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interaction Features : Interaction features are created by combining two or more features.\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"P1\",1, 500), (\"P2\",2, 200), (\"P3\",3, 300), (\"P4\",4, 400)]\n",
    "columns = [\"Pname\",\"Units_Sold\", \"Price_Per_Unit\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Create an interaction feature: Total_Sales_Value = Units_Sold * Price_Per_Unit\n",
    "df_interaction = df.withColumn(\"Total_Sales_Value\", col(\"Units_Sold\") * col(\"Price_Per_Unit\"))\n",
    "df_interaction.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
