{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-BJRVRTC3.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyAppName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=MyAppName>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc=pyspark.SparkContext(appName=\"MyAppName\")\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+------------------+\n",
      "|Pid|P_Name|Price|  Price_Normalized|\n",
      "+---+------+-----+------------------+\n",
      "|101|    P1|  450|               0.0|\n",
      "|102|    P2| 4034|0.8258064516129032|\n",
      "|103|    P3| 4790|               1.0|\n",
      "+---+------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max\n",
    "\n",
    "# Step 1: Start Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Price Normalization\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Sample data\n",
    "Product_Data = [(101, \"P1\", 450), (102, \"P2\", 4034), (103, \"P3\", 4790)]\n",
    "columns = [\"Pid\", \"P_Name\", \"Price\"]\n",
    "Product_DF = spark.createDataFrame(Product_Data, columns)\n",
    "\n",
    "# Step 3: Compute min and max\n",
    "Price_min = Product_DF.select(min(col(\"Price\"))).collect()[0][0]\n",
    "Price_max = Product_DF.select(max(col(\"Price\"))).collect()[0][0]\n",
    "\n",
    "# Step 4: Apply Min-Max Normalization\n",
    "Product_DF_normalized = Product_DF.withColumn(\n",
    "    \"Price_Normalized\", \n",
    "    (col(\"Price\") - Price_min) / (Price_max - Price_min)\n",
    ")\n",
    "\n",
    "# Step 5: Show result\n",
    "Product_DF_normalized.show()\n",
    "\n",
    "# Step 6: Stop Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------------------+\n",
      "|Pid|P_Name|Price| Price_Standardized|\n",
      "+---+------+-----+-------------------+\n",
      "|101|    P1|  450|-1.1392504769565068|\n",
      "|102|    P2| 4034|0.40658762605161547|\n",
      "|103|    P3| 4790| 0.7326628509048912|\n",
      "+---+------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Standardize Price\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sample data\n",
    "Product_Data = [(101, \"P1\", 450), (102, \"P2\", 4034), (103, \"P3\", 4790)]\n",
    "columns = [\"Pid\", \"P_Name\", \"Price\"]\n",
    "\n",
    "Product_DF = spark.createDataFrame(Product_Data, columns)\n",
    "\n",
    "# Calculate Mean and Standard Deviation\n",
    "Price_mean = Product_DF.select(mean(col(\"Price\"))).collect()[0][0]\n",
    "Price_stddev = Product_DF.select(stddev(col(\"Price\"))).collect()[0][0]\n",
    "\n",
    "# Standardize\n",
    "Product_DF_standardized = Product_DF.withColumn(\n",
    "    \"Price_Standardized\",\n",
    "    (col(\"Price\") - Price_mean) / Price_stddev\n",
    ")\n",
    "\n",
    "# Show result\n",
    "Product_DF_standardized.show()\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Salary_Grade|\n",
      "+------------+\n",
      "|     Class A|\n",
      "|     Class B|\n",
      "|     Class C|\n",
      "|     Class C|\n",
      "|     Class A|\n",
      "|     Class B|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyAppName\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "salarydata = [(\"Class A\",), (\"Class B\",), (\"Class C\",), (\"Class C\",), (\"Class A\",), (\"Class B\",)]\n",
    "columns = [\"Salary_Grade\"]\n",
    "\n",
    "salary_df = spark.createDataFrame(salarydata, columns)\n",
    "salary_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|Salary_Grade|Salary_Grade_Index|\n",
      "+------------+------------------+\n",
      "|     Class A|               0.0|\n",
      "|     Class B|               1.0|\n",
      "|     Class C|               2.0|\n",
      "|     Class C|               2.0|\n",
      "|     Class A|               0.0|\n",
      "|     Class B|               1.0|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#label encoding\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Initialize StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Salary_Grade\", outputCol=\"Salary_Grade_Index\")\n",
    "\n",
    "# Fit and Transform the DataFrame\n",
    "salary_df_indexed = indexer.fit(salary_df).transform(salary_df)\n",
    "salary_df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+---------------------------+\n",
      "|Salary_Grade|Salary_Grade_Index|Salary_Grade_OneHotEncoding|\n",
      "+------------+------------------+---------------------------+\n",
      "|     Class A|               0.0|              (2,[0],[1.0])|\n",
      "|     Class B|               1.0|              (2,[1],[1.0])|\n",
      "|     Class C|               2.0|                  (2,[],[])|\n",
      "|     Class C|               2.0|                  (2,[],[])|\n",
      "|     Class A|               0.0|              (2,[0],[1.0])|\n",
      "|     Class B|               1.0|              (2,[1],[1.0])|\n",
      "+------------+------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#one hot encoding\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCol=\"Salary_Grade_Index\", outputCol=\"Salary_Grade_OneHotEncoding\")\n",
    "\n",
    "# Transform the DataFrame\n",
    "salary_df_encoded = encoder.fit(salary_df_indexed).transform(salary_df_indexed)\n",
    "salary_df_encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------------+\n",
      "|Pid|P_Name|Price|Price_Squared|\n",
      "+---+------+-----+-------------+\n",
      "|101|    P1|  450|     202500.0|\n",
      "|102|    P2| 4034|  1.6273156E7|\n",
      "|103|    P3| 4790|    2.29441E7|\n",
      "+---+------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"PolynomialFeature\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "Product_Data = [(101, \"P1\", 450), (102, \"P2\", 4034), (103, \"P3\", 4790)]\n",
    "columns = [\"Pid\", \"P_Name\", \"Price\"]\n",
    "Product_DF = spark.createDataFrame(Product_Data, columns)\n",
    "\n",
    "# Feature Engineering: Create Price^2\n",
    "df_new = Product_DF.withColumn(\"Price_Squared\", col(\"Price\") ** 2)\n",
    "\n",
    "# Show results\n",
    "df_new.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------+-----------------+\n",
      "|Pname|Units_Sold|Price_Per_Unit|Total_Sales_Value|\n",
      "+-----+----------+--------------+-----------------+\n",
      "|   P1|         1|           500|              500|\n",
      "|   P2|         2|           200|              400|\n",
      "|   P3|         3|           300|              900|\n",
      "|   P4|         4|           400|             1600|\n",
      "+-----+----------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interaction Features : Interaction features are created by combining two or more features.\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"P1\",1, 500), (\"P2\",2, 200), (\"P3\",3, 300), (\"P4\",4, 400)]\n",
    "columns = [\"Pname\",\"Units_Sold\", \"Price_Per_Unit\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Create an interaction feature: Total_Sales_Value = Units_Sold * Price_Per_Unit\n",
    "df_interaction = df.withColumn(\"Total_Sales_Value\", col(\"Units_Sold\") * col(\"Price_Per_Unit\"))\n",
    "df_interaction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 – Student & Class DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+---+------+\n",
      "|rollno|   name|address|age|gender|\n",
      "+------+-------+-------+---+------+\n",
      "|     1|  Alice|   Pune| 23|     F|\n",
      "|     2|    Bob| Mumbai| 21|     M|\n",
      "|     3|Charlie| Nagpur| 24|     M|\n",
      "|     4|  Daisy|   Pune| 20|     F|\n",
      "|     5|  Ethan|  Delhi| 22|     M|\n",
      "+------+-------+-------+---+------+\n",
      "\n",
      "+-------+--------------+\n",
      "|classid|     classname|\n",
      "+-------+--------------+\n",
      "|    101|  Data Science|\n",
      "|    102|            AI|\n",
      "|    103|            ML|\n",
      "|    104|Cyber Security|\n",
      "|    105|           IoT|\n",
      "+-------+--------------+\n",
      "\n",
      "Male Students:\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|    Bob|\n",
      "|Charlie|\n",
      "|  Ethan|\n",
      "+-------+\n",
      "\n",
      "Female Students:\n",
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Alice|\n",
      "|Daisy|\n",
      "+-----+\n",
      "\n",
      "Students of classid 101:\n",
      "+------+----+-------+---+------+-------+---------+\n",
      "|rollno|name|address|age|gender|classid|classname|\n",
      "+------+----+-------+---+------+-------+---------+\n",
      "+------+----+-------+---+------+-------+---------+\n",
      "\n",
      "Students with age > 22:\n",
      "+------+-------+-------+---+------+\n",
      "|rollno|   name|address|age|gender|\n",
      "+------+-------+-------+---+------+\n",
      "|     1|  Alice|   Pune| 23|     F|\n",
      "|     3|Charlie| Nagpur| 24|     M|\n",
      "+------+-------+-------+---+------+\n",
      "\n",
      "+------+-------+-------+---+------+-----+\n",
      "|rollno|   name|address|age|gender|grade|\n",
      "+------+-------+-------+---+------+-----+\n",
      "|     1|  Alice|   Pune| 23|     F|   46|\n",
      "|     2|    Bob| Mumbai| 21|     M|   42|\n",
      "|     3|Charlie| Nagpur| 24|     M|   48|\n",
      "|     4|  Daisy|   Pune| 20|     F|   40|\n",
      "|     5|  Ethan|  Delhi| 22|     M|   44|\n",
      "+------+-------+-------+---+------+-----+\n",
      "\n",
      "+---------+-------+\n",
      "|classname|   name|\n",
      "+---------+-------+\n",
      "|     NULL|  Alice|\n",
      "|     NULL|    Bob|\n",
      "|     NULL|Charlie|\n",
      "|     NULL|  Daisy|\n",
      "|     NULL|  Ethan|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StudentClass\").getOrCreate()\n",
    "\n",
    "# Create student DataFrame\n",
    "student_data = [\n",
    "    (1, \"Alice\", \"Pune\", 23, \"F\"),\n",
    "    (2, \"Bob\", \"Mumbai\", 21, \"M\"),\n",
    "    (3, \"Charlie\", \"Nagpur\", 24, \"M\"),\n",
    "    (4, \"Daisy\", \"Pune\", 20, \"F\"),\n",
    "    (5, \"Ethan\", \"Delhi\", 22, \"M\")\n",
    "]\n",
    "student_columns = [\"rollno\", \"name\", \"address\", \"age\", \"gender\"]\n",
    "student_df = spark.createDataFrame(student_data, student_columns)\n",
    "\n",
    "# Create class DataFrame\n",
    "class_data = [\n",
    "    (101, \"Data Science\"),\n",
    "    (102, \"AI\"),\n",
    "    (103, \"ML\"),\n",
    "    (104, \"Cyber Security\"),\n",
    "    (105, \"IoT\")\n",
    "]\n",
    "class_columns = [\"classid\", \"classname\"]\n",
    "class_df = spark.createDataFrame(class_data, class_columns)\n",
    "\n",
    "# Display DataFrames\n",
    "student_df.show()\n",
    "class_df.show()\n",
    "\n",
    "# Display student names with gender = M and F separately\n",
    "print(\"Male Students:\")\n",
    "student_df.filter(col(\"gender\") == \"M\").select(\"name\").show()\n",
    "\n",
    "print(\"Female Students:\")\n",
    "student_df.filter(col(\"gender\") == \"F\").select(\"name\").show()\n",
    "\n",
    "# Display students of a specific class (Example: classid = 101)\n",
    "print(\"Students of classid 101:\")\n",
    "student_df.join(class_df, student_df.rollno == class_df.classid, \"inner\").show()\n",
    "\n",
    "# Students whose age > 22\n",
    "print(\"Students with age > 22:\")\n",
    "student_df.filter(col(\"age\") > 22).show()\n",
    "\n",
    "# Add new column grade\n",
    "student_df = student_df.withColumn(\"grade\", col(\"age\") * 2)  # sample logic\n",
    "student_df.show()\n",
    "\n",
    "# Display classnames and its associated students\n",
    "student_with_class = student_df.join(class_df, student_df.rollno == class_df.classid, \"left\")\n",
    "student_with_class.select(\"classname\", \"name\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '–' (U+2013) (1650815831.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mQuestion 2 – Employee & Department DataFrames\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '–' (U+2013)\n"
     ]
    }
   ],
   "source": [
    "Question 2 – Employee & Department DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eno: long (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- designation: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- dno: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- dno: long (nullable = true)\n",
      " |-- dname: string (nullable = true)\n",
      "\n",
      "+---+-----+------+-----------+------+------+---+\n",
      "|eno|ename|gender|designation|  city|salary|dno|\n",
      "+---+-----+------+-----------+------+------+---+\n",
      "|  2|Priya|     F|    Analyst|Mumbai| 18000| 20|\n",
      "|  5| Ravi|     M|    Analyst|Nagpur| 16000| 20|\n",
      "+---+-----+------+-----------+------+------+---+\n",
      "\n",
      "+---+-----+------+-----------+----+------+---+\n",
      "|eno|ename|gender|designation|city|salary|dno|\n",
      "+---+-----+------+-----------+----+------+---+\n",
      "|  1| John|     M|    Manager|Pune| 25000| 10|\n",
      "|  4| Neha|     F|    Manager|Pune| 27000| 30|\n",
      "+---+-----+------+-----------+----+------+---+\n",
      "\n",
      "+---+---+-----+------+-----------+------+------+-------+\n",
      "|dno|eno|ename|gender|designation|  city|salary|  dname|\n",
      "+---+---+-----+------+-----------+------+------+-------+\n",
      "| 20|  2|Priya|     F|    Analyst|Mumbai| 18000|Finance|\n",
      "| 30|  4| Neha|     F|    Manager|  Pune| 27000|     IT|\n",
      "+---+---+-----+------+-----------+------+------+-------+\n",
      "\n",
      "+---+-----+------+-----------+------+------+---+\n",
      "|eno|ename|gender|designation|  city|salary|dno|\n",
      "+---+-----+------+-----------+------+------+---+\n",
      "|  1| John|     M|    Manager|  Pune| 30000| 10|\n",
      "|  2|Priya|     F|    Analyst|Mumbai| 18000| 20|\n",
      "|  3|Karan|     M|      Clerk| Delhi| 15000| 10|\n",
      "|  4| Neha|     F|    Manager|  Pune| 32000| 30|\n",
      "|  5| Ravi|     M|    Analyst|Nagpur| 16000| 20|\n",
      "+---+-----+------+-----------+------+------+---+\n",
      "\n",
      "root\n",
      " |-- eno: long (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- designation: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- dno: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- dno: long (nullable = true)\n",
      " |-- dname: string (nullable = true)\n",
      "\n",
      "+---+---+-----+------+-----------+------+------+-------+\n",
      "|dno|eno|ename|gender|designation|  city|salary|  dname|\n",
      "+---+---+-----+------+-----------+------+------+-------+\n",
      "| 10|  1| John|     M|    Manager|  Pune| 30000|     HR|\n",
      "| 10|  3|Karan|     M|      Clerk| Delhi| 15000|     HR|\n",
      "| 10|  6|Divya|     F|      Clerk|Nashik| 14000|     HR|\n",
      "| 20|  2|Priya|     F|    Analyst|Mumbai| 18000|Finance|\n",
      "| 20|  5| Ravi|     M|    Analyst|Nagpur| 16000|Finance|\n",
      "| 20|  8|Sneha|     F|    Analyst|  Pune| 19000|Finance|\n",
      "| 30|  4| Neha|     F|    Manager|  Pune| 32000|     IT|\n",
      "| 30|  7| Amit|     M|    Manager|Mumbai| 30000|     IT|\n",
      "+---+---+-----+------+-----------+------+------+-------+\n",
      "\n",
      "+-------+-----+\n",
      "|  dname|ename|\n",
      "+-------+-----+\n",
      "|     HR| John|\n",
      "|     HR|Karan|\n",
      "|     HR|Divya|\n",
      "|Finance|Priya|\n",
      "|Finance| Ravi|\n",
      "|Finance|Sneha|\n",
      "|     IT| Neha|\n",
      "|     IT| Amit|\n",
      "+-------+-----+\n",
      "\n",
      "+---+-----+------+-----------+------+------+---+\n",
      "|eno|ename|gender|designation|  city|salary|dno|\n",
      "+---+-----+------+-----------+------+------+---+\n",
      "|  2|Priya|     F|    Analyst|Mumbai| 18000| 20|\n",
      "|  5| Ravi|     M|    Analyst|Nagpur| 16000| 20|\n",
      "|  8|Sneha|     F|    Analyst|  Pune| 19000| 20|\n",
      "+---+-----+------+-----------+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create emp and dept DataFrames\n",
    "emp_data = [\n",
    "    (1, \"John\", \"M\", \"Manager\", \"Pune\", 25000, 10),\n",
    "    (2, \"Priya\", \"F\", \"Analyst\", \"Mumbai\", 18000, 20),\n",
    "    (3, \"Karan\", \"M\", \"Clerk\", \"Delhi\", 15000, 10),\n",
    "    (4, \"Neha\", \"F\", \"Manager\", \"Pune\", 27000, 30),\n",
    "    (5, \"Ravi\", \"M\", \"Analyst\", \"Nagpur\", 16000, 20)\n",
    "]\n",
    "emp_columns = [\"eno\", \"ename\", \"gender\", \"designation\", \"city\", \"salary\", \"dno\"]\n",
    "emp_df = spark.createDataFrame(emp_data, emp_columns)\n",
    "\n",
    "dept_data = [\n",
    "    (10, \"HR\"),\n",
    "    (20, \"Finance\"),\n",
    "    (30, \"IT\"),\n",
    "    (40, \"Marketing\"),\n",
    "    (50, \"Admin\")\n",
    "]\n",
    "dept_columns = [\"dno\", \"dname\"]\n",
    "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
    "\n",
    "# Print schema\n",
    "emp_df.printSchema()\n",
    "dept_df.printSchema()\n",
    "\n",
    "# Filter based on designation and salary\n",
    "emp_df.filter(col(\"designation\") == \"Analyst\").show()\n",
    "emp_df.filter(col(\"salary\") > 20000).show()\n",
    "\n",
    "# Departments for female employees\n",
    "emp_df.filter(col(\"gender\") == \"F\").join(dept_df, \"dno\").show()\n",
    "\n",
    "# Increase salary of Managers\n",
    "emp_df = emp_df.withColumn(\"salary\", \n",
    "                           col(\"salary\") + 5000 * (col(\"designation\") == \"Manager\").cast(\"int\"))\n",
    "emp_df.show()\n",
    "\n",
    "# Add 3 more records to each dataframe\n",
    "more_emp = [\n",
    "    (6, \"Divya\", \"F\", \"Clerk\", \"Nashik\", 14000, 10),\n",
    "    (7, \"Amit\", \"M\", \"Manager\", \"Mumbai\", 30000, 30),\n",
    "    (8, \"Sneha\", \"F\", \"Analyst\", \"Pune\", 19000, 20)\n",
    "]\n",
    "more_dept = [\n",
    "    (60, \"Legal\"),\n",
    "    (70, \"Tech\"),\n",
    "    (80, \"Logistics\")\n",
    "]\n",
    "\n",
    "emp_df = emp_df.union(spark.createDataFrame(more_emp, emp_columns))\n",
    "dept_df = dept_df.union(spark.createDataFrame(more_dept, dept_columns))\n",
    "\n",
    "emp_df.printSchema()\n",
    "dept_df.printSchema()\n",
    "\n",
    "# Use join to fetch unique records\n",
    "emp_df.join(dept_df, \"dno\").dropDuplicates().show()\n",
    "\n",
    "# Department-wise list of employees\n",
    "emp_df.join(dept_df, \"dno\").select(\"dname\", \"ename\").show()\n",
    "\n",
    "# Employees with salary < 20000 and designation = 'Analyst'\n",
    "emp_df.filter((col(\"salary\") < 20000) & (col(\"designation\") == \"Analyst\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Question 3 – Product, Customer, Orders, Order_Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+----+\n",
      "|cust_id|cname|  mobileno|city|\n",
      "+-------+-----+----------+----+\n",
      "|      1| Asha|9999999999|Pune|\n",
      "|      3|Meena|7777777777|Pune|\n",
      "+-------+-----+----------+----+\n",
      "\n",
      "+-------------------+---------------------+-------------------+-------------------+--------+----------+-----------------+------------+\n",
      "|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_id|order_date|order_customer_id|order_status|\n",
      "+-------------------+---------------------+-------------------+-------------------+--------+----------+-----------------+------------+\n",
      "|                102|                    3|                  1|                500|     102|2013-08-15|                2|      CLOSED|\n",
      "+-------------------+---------------------+-------------------+-------------------+--------+----------+-----------------+------------+\n",
      "\n",
      "+-------------------+---------------------+-------------------+-------------------+\n",
      "|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|\n",
      "+-------------------+---------------------+-------------------+-------------------+\n",
      "|                101|                    1|                  2|                 20|\n",
      "|                104|                    1|                  3|                 30|\n",
      "|                101|                    2|                  1|                 40|\n",
      "|                102|                    3|                  1|                500|\n",
      "+-------------------+---------------------+-------------------+-------------------+\n",
      "\n",
      "+-----+-----+\n",
      "|cname|total|\n",
      "+-----+-----+\n",
      "|Rahul|  500|\n",
      "|Rohan|   30|\n",
      "+-----+-----+\n",
      "\n",
      "+--------+----------+-----------------+------------+\n",
      "|order_id|order_date|order_customer_id|order_status|\n",
      "+--------+----------+-----------------+------------+\n",
      "|     101|2013-08-01|                1|    COMPLETE|\n",
      "|     102|2013-08-15|                2|      CLOSED|\n",
      "|     104|2013-08-01|                4|    COMPLETE|\n",
      "+--------+----------+-----------------+------------+\n",
      "\n",
      "+--------+----------+-----------------+------------+\n",
      "|order_id|order_date|order_customer_id|order_status|\n",
      "+--------+----------+-----------------+------------+\n",
      "|     101|2013-08-01|                1|    COMPLETE|\n",
      "|     102|2013-08-15|                2|      CLOSED|\n",
      "|     104|2013-08-01|                4|    COMPLETE|\n",
      "+--------+----------+-----------------+------------+\n",
      "\n",
      "+-------------------+---------------------+-------------------+-------------------+----------+-----+-----+-----+\n",
      "|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|product_id|pname|ptype|price|\n",
      "+-------------------+---------------------+-------------------+-------------------+----------+-----+-----+-----+\n",
      "+-------------------+---------------------+-------------------+-------------------+----------+-----+-----+-----+\n",
      "\n",
      "+--------+----------+-----------------+------------+\n",
      "|order_id|order_date|order_customer_id|order_status|\n",
      "+--------+----------+-----------------+------------+\n",
      "|     101|2013-08-01|                1|    COMPLETE|\n",
      "|     103|2013-07-01|                3|     PENDING|\n",
      "|     104|2013-08-01|                4|    COMPLETE|\n",
      "+--------+----------+-----------------+------------+\n",
      "\n",
      "+------------+-----+\n",
      "|order_status|count|\n",
      "+------------+-----+\n",
      "|    COMPLETE|    2|\n",
      "|      CLOSED|    1|\n",
      "|     PENDING|    1|\n",
      "+------------+-----+\n",
      "\n",
      "+-------------------+-------+\n",
      "|order_item_order_id|revenue|\n",
      "+-------------------+-------+\n",
      "|                101|     60|\n",
      "|                102|    500|\n",
      "|                104|     30|\n",
      "+-------------------+-------+\n",
      "\n",
      "+----------+---------------------+-------------+\n",
      "|order_date|order_item_product_id|daily_revenue|\n",
      "+----------+---------------------+-------------+\n",
      "|2013-08-01|                    1|           50|\n",
      "|2013-08-01|                    2|           40|\n",
      "|2013-08-15|                    3|          500|\n",
      "+----------+---------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month, year, min, max, count, sum, to_date, dayofmonth\n",
    "\n",
    "# Create DataFrames\n",
    "product_data = [\n",
    "    (1, \"Pen\", \"Stationery\", 10),\n",
    "    (2, \"Notebook\", \"Stationery\", 40),\n",
    "    (3, \"Mouse\", \"Electronics\", 500),\n",
    "    (4, \"Keyboard\", \"Electronics\", 700)\n",
    "]\n",
    "product_df = spark.createDataFrame(product_data, [\"product_id\", \"pname\", \"ptype\", \"price\"])\n",
    "\n",
    "customer_data = [\n",
    "    (1, \"Asha\", \"9999999999\", \"Pune\"),\n",
    "    (2, \"Rahul\", \"8888888888\", \"Mumbai\"),\n",
    "    (3, \"Meena\", \"7777777777\", \"Pune\"),\n",
    "    (4, \"Rohan\", \"6666666666\", \"Nagpur\")\n",
    "]\n",
    "customer_df = spark.createDataFrame(customer_data, [\"cust_id\", \"cname\", \"mobileno\", \"city\"])\n",
    "\n",
    "orders_data = [\n",
    "    (101, \"2013-08-01\", 1, \"COMPLETE\"),\n",
    "    (102, \"2013-08-15\", 2, \"CLOSED\"),\n",
    "    (103, \"2013-07-01\", 3, \"PENDING\"),\n",
    "    (104, \"2013-08-01\", 4, \"COMPLETE\")\n",
    "]\n",
    "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\"])\n",
    "\n",
    "order_items_data = [\n",
    "    (101, 1, 2, 20),\n",
    "    (101, 2, 1, 40),\n",
    "    (102, 3, 1, 500),\n",
    "    (104, 1, 3, 30)\n",
    "]\n",
    "order_items_df = spark.createDataFrame(order_items_data, \n",
    "    [\"order_item_order_id\", \"order_item_product_id\", \"order_item_quantity\", \"order_item_subtotal\"])\n",
    "\n",
    "# Get customers from Pune\n",
    "customer_df.filter(col(\"city\") == \"Pune\").show()\n",
    "\n",
    "# Orders with subtotal > 100 in August\n",
    "orders_df = orders_df.withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "order_items_df.join(orders_df, order_items_df.order_item_order_id == orders_df.order_id)\\\n",
    "    .filter((col(\"order_item_subtotal\") > 100) & (month(\"order_date\") == 8))\\\n",
    "    .show()\n",
    "\n",
    "# Orders in ascending order of subtotal\n",
    "order_items_df.orderBy(\"order_item_subtotal\").show()\n",
    "\n",
    "# Customer with min and max order\n",
    "order_totals = order_items_df.groupBy(\"order_item_order_id\").agg(sum(\"order_item_subtotal\").alias(\"total\"))\n",
    "min_max = order_totals.agg(min(\"total\").alias(\"min\"), max(\"total\").alias(\"max\")).collect()\n",
    "min_val, max_val = min_max[0][\"min\"], min_max[0][\"max\"]\n",
    "\n",
    "orders_df.join(order_totals, orders_df.order_id == order_totals.order_item_order_id)\\\n",
    "    .filter((col(\"total\") == min_val) | (col(\"total\") == max_val))\\\n",
    "    .join(customer_df, orders_df.order_customer_id == customer_df.cust_id)\\\n",
    "    .select(\"cname\", \"total\").show()\n",
    "\n",
    "# Orders with status COMPLETE or CLOSED\n",
    "orders_df.filter(col(\"order_status\").isin(\"COMPLETE\", \"CLOSED\")).show()\n",
    "\n",
    "# Orders in Aug 2013 with COMPLETE or CLOSED\n",
    "orders_df.filter(\n",
    "    (col(\"order_status\").isin(\"COMPLETE\", \"CLOSED\")) & \n",
    "    (month(\"order_date\") == 8) & (year(\"order_date\") == 2013)\n",
    ").show()\n",
    "\n",
    "# Wrong subtotal check\n",
    "wrong_subtotal = order_items_df.join(product_df, order_items_df.order_item_product_id == product_df.product_id)\n",
    "wrong_subtotal.filter(col(\"order_item_subtotal\") != (col(\"order_item_quantity\") * col(\"price\"))).show()\n",
    "\n",
    "# Orders placed on first of every month\n",
    "orders_df.filter(dayofmonth(\"order_date\") == 1).show()\n",
    "\n",
    "# Count by status\n",
    "orders_df.groupBy(\"order_status\").agg(count(\"*\").alias(\"count\")).show()\n",
    "\n",
    "# Revenue per order id\n",
    "order_items_df.groupBy(\"order_item_order_id\").agg(sum(\"order_item_subtotal\").alias(\"revenue\")).show()\n",
    "\n",
    "# Daily product revenue\n",
    "orders_df.join(order_items_df, orders_df.order_id == order_items_df.order_item_order_id)\\\n",
    "    .groupBy(\"order_date\", \"order_item_product_id\")\\\n",
    "    .agg(sum(\"order_item_subtotal\").alias(\"daily_revenue\")).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
